% CREATED BY DAVID FRISK, 2016
\chapter{Theory}
\section{Grounding}
%A key challenge in creating robots that can interact with humans in natural language is grounding--connecting an external object to a word or phrase. 
The challenge of grounding is key in creating robots that can interact with humans in natural language. The robot must be able to link objects and actions to words. For example, what does 'turn' mean? Grounding is a challenge in a variety of tasks, such as image captioning, where models need to produce labels for both the objects in the image and the objects' interactions with each other in natural language \cite{karpathy2014captioning}. % SD 2020-12-11 21:25:12 +0100: Actually, the notion of grounding language in perception has been explored in robotics first and then moved into image captioning which is a much younger task: https://canvas.gu.se/courses/36167/assignments/75554 and https://canvas.gu.se/courses/36167/assignments/76897
Image captioning is a fairly new task, but grounding in the field of robotics has been an area of active research for a long time. In 2002, Lauria et al. presented a project in which a robot was given natural language instructions, which were mapped to procedures of identified primitives, such as 'turn' \cite{lauria2002}. More recently, Hermann et al. presented a simulated embodied agent learning grounded language through a combination of unsupervised and reinforcement learning, with minimal prior knowledge \cite{hermann2017grounded}. Another approach with minimal prior knowledge was presented by Thomason et al., where the robot learns through dialogue with a human \cite{thomason2019grounded}. \newline
\section{Visual Question Answering}
Visual Question Answering is a task in which an agent must answer a question based on an image; these questions are "free-form and open-ended" \cite{vqa_2015}. This task differs from image captioning in that answering these questions requires retrieving specific information, rather than simply identification % identification
of things/activities in the image. There is a huge variety in possible question types in this task, including object detection ("how many...?"), activity recognition ("Is this person doing ...?"), and knowledge-base reasoning ("what is ... made of?"). % SD 2021-05-01 14:26:34 +0200: where is the book on the renaissance painting?
VQA is  a somewhat unusual natural language processing task in that it is often implemented as a classification task, with the agent choosing from a large number of potential answers, rather than generating an answer. % SD 2021-05-01 14:28:10 +0200: There are now some approaches that do generation.
This means that the success of VQA can often be measured as accuracy to the ground truth answer. However, VQA can also be implemented as a generation task, for which other measures, like BLEU, which compares a generated output to one or more good human outputs, can be used \cite{vqa_survey}\cite{bleu}. \newline
\section{Navigation}
Navigation is a common task for robots. \cite{visualnavigationsurvey} describes it as "the process of determining a suitable and safe path between a starting and a goal point for a robot travelling between them". A variety of navigation strategies exist, including map based strategies and strategies involving visual landmarks. \newline
One navigation task is Object Navigation, in which the agent, given an object's label, navigates to the object \cite{objectnavrevisited}. \newline
\section{Embodied Question Answering}
Embodied Question Answering combines the navigation and VQA tasks into one: the embodied agent must navigate to find the object that the question refers to \cite{embodiedqa}. For example, for the question 'What color is the refrigerator?' the agent must identify where the fridge is most likely to be, the kitchen, and navigate there before identifying the fridge and answering the color. This task has been expanded to Multi-Target Embodied Question Answering, in which questions can include multiple targets, allowing for comparison questions such as 'is the oven in the kitchen the same color as the sink in the bathroom?' \cite{eqa_multitarget}. Current approaches for this task use templated approaches for generating questions, due to the costs of collecting a large enough dataset from humans.  % SD 2021-05-01 14:29:34 +0200:  This is becauce collecting a sufficiently large coprus of human questions and answers is expensive both in time and financially.
The EQA dataset includes nine question types: location, color, color\_room, preposition, existence, logical, count, room\_count, and distance, though the EQA-V1 dataset, used for the experiments by Das et.al includes only the first five question types \cite{embodiedqa}. The MT-EQA dataset adds six comparison question types \cite{eqa_multitarget}. \newline
One concern with EQA models is how much they actually incorporate the visual input in determining an answer. Anand et al conducted an experiment on the EQAv1 dataset that found equivalent to slightly better performance on the question answering task using simple question only models with no visual input \cite{blindfolded}. This is interesting in that it suggests that the models are doing a good job learning common sense knowledge from the textual content, but is a problem in this particular task, since it means that the agent is not actually adapting its answers to the specific situation. \newline
\subsection{Dialogue}
VQA and EQA can be seen as simplified dialogue tasks. They contain a limited number of question types, and the agent sticks to answering, rather than employing more sophisticated strategies, such as asking questions to acquire more information when it is unsure. One interesting note about dialogue interactions between humans and robots is that humans automatically adjust their strategies when interacting with a computer. % SD 2021-05-01 14:59:47 +0200: They also do this when they communicate with each other: [1] H. H. Clark and D. Wilkes-Gibbs. Referring as a collaborative process. Cognition, 22(1):1–39, 1986. [2] H. H. Clark. Using language. Cambridge University Press, Cambridge, 1996. We would like machines to be equally adaptable. But currently in most cases the adaptation is one-sided, from a human side only.
% NI 2021-05-17: you can cite/describe some work on visual dialogue here (or, at least, connect it to the VQA/EQA): Das et al 2017 (introduce the task of visual dialogue), Where Are You? Localization from Embodied Dialog (Anderson 2021) propose some initial and simple visdial implementations, MeetUp (Ilinykh et al 2019) and PhotoBook dataset (https://arxiv.org/pdf/1906.01530.pdf) is also about visually grounded dialogue.
Tenbrink et al found that people gave generally sparser commands when interacting with a computer system than they did interacting with another human, even when not given instructions to do so \cite{Tenbrink:2010qf}. \newline
A current area of research is Visual Dialog, in which an agent holds a conversation with a human about some visual content \cite{das2017}. A relevant dataset for visual dialogue in an interior setting is Meet Up!, a corpus of dialogue and images from where two people played a game in a simulator--the participants were dropped into two separate rooms, and had to navigate to each other by describing the rooms that they were seeing \cite{meetup}. A similar dataset is Where Are You (WAY), in which one participant is the Observer, who has a first person view of the space, and the Locator, who must determine where the Observer is in a top down map by asking the Observer questions \cite{whereareyou}. This task leads into another area of research, navigation based on dialogue, in which the agent navigates based on language instructions, and can ideally ask questions for clarification (making it a dialogue). \todo{Add R2R, figure out exactly how it relates to CVDN} Like visual dialogue, datasets for dialogue based navigation are often collected by having humans play both roles. Cooperative Vision-and-Dialog Navigation is a dataset of over 2000 navigation dialogues \cite{thomason2019visionanddialog}. It was collected via Mechanical Turk crowd-sourcing; pairs used the Matterport3D simulator and a chat interface, with one person, the ’oracle’, able to see the ideal moves for the navigation task, giving natural language instructions to the other person, the ’navigator'. The navigator could ask clarifying questions. The oracle was also shown the navigator's current visual frame. \newline
A similar but smaller dataset is RobotSlang, consisting of 169 dialogues between a commander referencing a static map and a human driver only able to see the camera view of the robot they were controlling \cite{robotslang}. The commander relayed instructions to the driver, based on their understanding of where the robot was. The driver was able to ask clarifying questions, such as where exactly to turn, and the commander could periodically ask localization questions, such as what color wall the driver could see. \newline
\section{Simulation}
Working with embodied agents is resource intensive and makes reproducibility difficult to impossible, so simulation is beneficial for research. Within simulation, environments can be kept consistent, allowing for both reproducibility of an experiment and for comparison of different systems or methods. Simulation also allows for the reuse of datasets of human descriptions or labels, which are time-consuming and expensive to produce. \newline
\todo{other platforms, other datasets}
AI Habitat is a simulation platform for working with embodied AI \cite{habitat19iccv}. It consists of two parts, Habitat-Sim, the 3D simulator, and Habitat-Lab, the library for embodied AI development. Habitat's current main focus is navigation, mainly through indoor spaces, but there is some ability for object interactions--for example moving a chair from one point to another. New objects can also be added to the space. Habitat-PyRobot Bridge is a library, written by members of the Habitat team, to support the transfer of a simulated agent in Habitat to a physical robot \cite{Kadian_2020}. Various scene datasets are supported by Habitat, the most used one being Matterport3D, a dataset of real interiors with human annotation of objects \cite{matterport}. Other datasets are also available, such as Replica, which is smaller than Matterport, but more realistic due to cleaner rendering \cite{replica}. 

% SD 2021-05-01 15:04:06 +0200: Recently discovered: it is also possible to introduce new objects and move them in space.

% SD 2021-05-01 15:04:56 +0200: Here you could also describe how we simulate language and navigtaion: by generating questions and answers.

\section{Research Questions}
% SD 2021-05-01 16:41:31 +0200: Overall, the report looks very good and now we certainly have a lot iof interesting research questions to answer. The comments I gave are general for the future thesis work; I'm not sure how much detail you need to provide in the intermediate report; you can certainly outline the directions we will be taking. One thing that an examiner mght say is that the report does not link these research questions more strongly together and explain how they relate to the overall goal of the thesis. Here, my suggestion is as follows:

% The overall research goal of the thesis is to examines how we can improve EQA based on incorporating external knowledge and exmaine the relation between different kinds of knowledge used for EQA.

% (1) The blindfolding examines the role of textual and visual information in the EQA task. How informative is each?

% (2) With inclusion of semantic labels of places we give a model a better categorical representation of the scene. We tell it what categroies of objects and surfaces are found there.

% (3) With DenseCap with infuse rich external common sense knowledge: visual features common sense and linguistic catgorical information about how we structure the world.

% (4) With look around: we give the model an ability to expand the search and filter out less relevant visual representations. For this we need to implement some model of attention.

% (5) With BERT we would focus on a more powerful architure to perform the task.

Embodied Question Answering is interesting in that it requires specific identifications--people's homes will have multiple tables, and if you are asking your embodied agent about a specific table, you need the agent to be able to identify it. However, it is also a situation where you do not want to have to train your agent from scratch in every new location. If we see this as a step towards a house or office assistance agent, the user cannot generate their own dataset for their location and then spend days training the agent in its current environment. The agent needs to be able to give specifics about objects it may never have seen before. So, we need an agent to be able to leverage information from other locations or contexts, while being able to be specific about the current location. The goal of this project is to determine:
\begin{itemize}
\item How to give the agent useful information, both from the current context and location, and from other contexts and locations
\item How linguistic and visual information can be combined to reach the desired level of specificity towards an object
\end{itemize}
