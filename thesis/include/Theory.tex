% CREATED BY DAVID FRISK, 2016
\chapter{Theory}
\section{Grounding}
\todo{copied from proposal, rework}
%A key challenge in creating robots that can interact with humans in natural language is grounding--connecting an external object to a word or phrase. 
The challenge of grounding is key in creating robots that can interact with humans in natural language. The robot must be able to link objects and actions to words. For example, what does 'turn' mean? Grounding is a challenge in a variety of tasks, such as image captioning, where models need to produce labels for both the objects in the image and the objects' interactions with each other in natural language\cite{karpathy2014captioning}.

% SD 2020-12-11 21:25:12 +0100: Actually, the notion of grounding language in perception has been explored in robotics first and then moved into image captioning which is a much younger task: https://canvas.gu.se/courses/36167/assignments/75554 and https://canvas.gu.se/courses/36167/assignments/76897
Image captioning is a fairly new task, but grounding in the field of robotics has been an area of active research for a long time. In 2002, Lauria et al. presented a project in which a robot was given natural language instructions, which were mapped to procedures of identified primitives, such as 'turn'\cite{lauria2002}. More recently, Hermann et al. presented a simulated embodied agent learning grounded language through a combination of unsupervised and reinforcement learning, with minimal prior knowledge\cite{hermann2017grounded}. Another approach with minimal prior knowledge was presented by Thomason et al., where the robot learns through dialogue with a human\cite{thomason2019grounded}.

\section{Visual Question Answering}
\todo{copied from proposal, rework.}
Visual Question Answering is a task in which an agent must answer a question based on an image; these questions are "free-form and open-ended"\cite{vqa_2015}. This task differs from image captioning in that answering these questions requires retrieving specific information, rather than simply identification % identification
of things/activities in the image. There is a huge variety in possible question types in this task, including object detection ("how many...?"), activity recognition ("Is this person doing ...?"), and knowledge-base reasoning ("what is ... made of?"). % SD 2021-05-01 14:26:34 +0200: where is the book on the renaissance painting?
VQA is  a somewhat unusual natural language processing task in that it is often implemented as a classification task, with the agent choosing from a large number of potential answers, rather than generating an answer. % SD 2021-05-01 14:28:10 +0200: There are now some approaches that do generation.
This means that the success of VQA can often be measured as accuracy to the ground truth answer. However, VQA can also be implemented as a generation task, for which other measures, like BLEU, which compares a generated output to one or more good human outputs, can be used\cite{bleu}.
\section{Embodied Question Answering}
\todo{copied from proposal, rework, add detail, particularly discussion of queston types and the kinds of information needed for them}
Embodied Question Answering combines the navigation and VQA tasks into one: the embodied agent must navigate to find the object that the question refers to\cite{embodiedqa}. For example, for the question 'What color is the refrigerator?' the agent must identify where the fridge is most likely to be, the kitchen, and navigate there before identifying the fridge and answering the color. This task has been expanded to Multi-Target Embodied Question Answering, in which questions can include multiple targets, allowing for comparison questions such as 'is the oven in the kitchen the same color as the sink in the bathroom?'\cite{eqa_multitarget}. Current approaches for this task use templated approaches for generating questions, due to the costs of collecting a large enough dataset from humans.  % SD 2021-05-01 14:29:34 +0200:  This is becauce collecting a sufficiently large coprus of human questions and answers is expensive both in time and financially.
The EQA dataset includes nine question types: location, color, color\_room, preposition, existence, logical, count, room\_count, and distance, though the EQA-V1 dataset, used for the experiments by Das et.al includes only the first five question types\cite{embodiedqa}. The MT-EQA dataset adds six comparison question types\cite{eqa_multitarget}.

One concern with EQA models is how much they actually incorporate the visual input in determining an answer. Anand et al conducted an experiment on the EQAv1 dataset that found equivalent to slightly better performance on the question answering task using simple question only models with no visual input\cite{blindfolded}. This is interesting in that it suggests that the models are doing a good job learning common sense knowledge from the textual content, but is a problem in this particular task, since it means that the agent is not actually adapting its answers to the specific situation. 

VQA and EQA can be seen as simplified dialogue tasks. They contain a limited number of question types, and the agent sticks to answering, rather than employing more sophisticated strategies, such as asking questions to acquire more information when it is unsure. One interesting note about dialogue interactions between humans and robots is that humans automatically adjust their strategies when interacting with a computer. % SD 2021-05-01 14:59:47 +0200: They also do this when they communicate with each other: [1] H. H. Clark and D. Wilkes-Gibbs. Referring as a collaborative process. Cognition, 22(1):1â€“39, 1986. [2] H. H. Clark. Using language. Cambridge University Press, Cambridge, 1996. We would like machines to be equally adaptable. But currently in most cases the adaptation is one-sided, from a human side only.
Tenbrink et al found that people gave generally sparser commands when interacting with a computer system than they did interacting with another human, even when not given instructions to do so\cite{Tenbrink:2010qf}. 

\section{Simulation}
\todo{copied from proposal, rework}
Working with embodied agents is resource intensive and makes reproducibility difficult to impossible, so simulation is beneficial for research. Within simulation, environments can be kept consistent, allowing for both reproducibility of an experiment and for comparison of different systems/methods. Simulation also allows for the reuse of datasets of human descriptions or labels, which are time-consuming and expensive to produce.

AI Habitat is a simulation platform for working with embodied AI\cite{habitat19iccv}. It consists of two parts, Habitat-Sim, the 3D simulator, and Habitat-Lab, the library for embodied AI development. Habitat's current main focus is navigation, mainly through indoor spaces, but there is some ability for object interactions--for example moving a chair from one point to another. New objects can also be added to the space. Habitat-PyRobot Bridge is a library, written by members of the Habitat team, to support the transfer of a simulated agent in Habitat to a physical robot\cite{Kadian_2020}. Various scene datasets are supported by Habitat, the largest of which is Matterport3D, a dataset of real interiors with human annotation of objects\cite{matterport}.

% SD 2021-05-01 15:04:06 +0200: Recently discovered: it is also possible to introduce new objects and move them in space.

% SD 2021-05-01 15:04:56 +0200: Here you could also describe how we simulate language and navigtaion: by generating questions and answers.

\section{Research Questions}
% SD 2021-05-01 16:41:31 +0200: Overall, the report looks very good and now we certainly have a lot iof interesting research questions to answer. The comments I gave are general for the future thesis work; I'm not sure how much detail you need to provide in the intermediate report; you can certainly outline the directions we will be taking. One thing that an examiner mght say is that the report does not link these research questions more strongly together and explain how they relate to the overall goal of the thesis. Here, my suggestion is as follows:

% The overall research goal of the thesis is to examines how we can improve EQA based on incorporating external knowledge and exmaine the relation between different kinds of knowledge used for EQA.

% (1) The blindfolding examines the role of textual and visual information in the EQA task. How informative is each?

% (2) With inclusion of semantic labels of places we give a model a better categorical representation of the scene. We tell it what categroies of objects and surfaces are found there.

% (3) With DenseCap with infuse rich external common sense knowledge: visual features common sense and linguistic catgorical information about how we structure the world.

% (4) With look around: we give the model an ability to expand the search and filter out less relevant visual representations. For this we need to implement some model of attention.

% (5) With BERT we would focus on a more powerful architure to perform the task.

Embodied Question Answering is interesting in that it requires specific identifications--people's homes will have multiple tables, and if you are asking your embodied agent about a specific table, you need the agent to be able to identify it. However, it is also a situation where you do not want to have to train your agent from scratch in every new location. If we see this as a step towards a house or office assistance agent, the user cannot generate their own dataset for their location and then spend days training the agent in its current environment. The agent needs to be able to give specifics about objects it may never have seen before. So, we need an agent to be able to leverage information from other locations or contexts, while being able to be specific about the current location. The goal of this project is to determine:
\begin{itemize}
\item how to give the agent useful information, both from the current context and location, and from other contexts and locations
\item how linguistic and visual information can be combined to reach the desired level of specificity towards an object
\end{itemize}
