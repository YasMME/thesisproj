% CREATED BY DAVID FRISK, 2016
\chapter{Background}
\section{Visual Question Answering}
\todo{copied from proposal, rework.}
Visual Question Answering is a task in which an agent must answer a question based on an image; these questions are "free-form and open-ended"\cite{vqa_2015}. This task differs from image captioning in that answering these questions requires retrieving specific information, rather than general % identification
description of things/activities in the image. There is a huge variety in possible question types in this task, including object detection ("how many...?"), activity recognition ("Is this person doing ...?"), and knowledge-base reasoning ("what is ... made of?"). % SD 2021-05-01 14:26:34 +0200: where is the book on the renaissance painting?
VQA is % a somewhat unusual natural language processing task in that it is
often implemented as a classification task, with the agent choosing from a large number of potential answers, rather than generating an answer. % SD 2021-05-01 14:28:10 +0200: There are now some approaches that do generation.
This means that the success of VQA can often be measured as accuracy to the ground truth answer. However, VQA can also be implemented as a generation task, for which other measures, like BLEU, which compares a generated output to one or more good human outputs, can be used\cite{bleu}.
\section{Embodied Question Answering}
\todo{copied from proposal, rework, add detail, particularly discussion of queston types and the kinds of information needed for them}
Embodied Question Answering combines the navigation and VQA tasks into one: the embodied agent must navigate to find the object that the question refers to\cite{embodiedqa}. For example, for the question 'What color is the refrigerator?' the agent must identify where the fridge is most likely to be, the kitchen, and navigate there before identifying the fridge and answering the color. This task has been expanded to Multi-Target Embodied Question Answering, in which questions can include multiple targets, allowing for comparison questions such as 'is the oven in the kitchen the same color as the sink in the bathroom?'\cite{eqa_multitarget}. Current approaches for this task use templated approaches for generating questions. % SD 2021-05-01 14:29:34 +0200:  This is becauce collecting a sufficiently large coprus of human questions and answers is expensive both in time and financially.
The EQA dataset includes nine question types: location, color, color\_room, preposition, existence, logical, count, room\_count, and distance, though the EQA-V1 dataset, used for the experiments by Das et.al includes only the first five question types\cite{embodiedqa}. The MT-EQA dataset adds six comparison question types\cite{eqa_multitarget}.

One concern with EQA models is how much they actually incorporate the visual input in determining an answer. Anand et al conducted an experiment on the EQAv1 dataset that found equivalent to slightly better performance on the question answering task using simple question only models with no visual input\cite{blindfolded}. This is interesting in that it suggests that the models are doing a good job learning common sense knowledge from the textual content, but is a problem in this particular task, since it means that the agent is not actually adapting its answers to the specific situation. 

VQA and EQA can be seen as simplified dialogue tasks. They contain a limited number of question types, and the agent sticks to answering, rather than employing more sophisticated strategies, such as asking questions to acquire more information when it is unsure. One interesting note about dialogue interactions between humans and robots is that humans automatically adjust their strategies when interacting with a computer. % SD 2021-05-01 14:59:47 +0200: They also do this when they communicate with each other: [1] H. H. Clark and D. Wilkes-Gibbs. Referring as a collaborative process. Cognition, 22(1):1â€“39, 1986. [2] H. H. Clark. Using language. Cambridge University Press, Cambridge, 1996. We would like machines to be equally adaptable. But currently in most cases the adaptation is one-sided, from a human side only.
Tenbrink et al found that people gave generally sparser commands when interacting with a computer system than they did interacting with another human, even when not given instructions to do so\cite{Tenbrink:2010qf}. 

\section{Simulation}
\todo{copied from proposal, rework}
Working with embodied agents is resource intensive and makes reproducibility difficult to impossible, so simulation is beneficial for research. Within simulation, environments can be kept consistent, allowing for both reproducibility of an experiment and for comparison of different systems/methods. Simulation also allows for the reuse of datasets of human descriptions or labels, which are time-consuming and expensive to produce.

AI Habitat is a simulation platform for working with embodied AI\cite{habitat19iccv}. It consists of two parts, Habitat-Sim, the 3D simulator, and Habitat-Lab, the library for embodied AI development. Habitat's current main focus is navigation, mainly through indoor spaces, but there is some ability for object interactions--for example moving a chair from one point to another. Habitat-PyRobot Bridge is a library, written by members of the Habitat team, to support the transfer of a simulated agent in Habitat to a physical robot\cite{Kadian_2020}. Various scene datasets are supported by Habitat, the largest of which is Matterport3D, a dataset of real interiors with human annotation of objects\cite{matterport}.

% SD 2021-05-01 15:04:06 +0200: Recently discovered: it is also possible to introduce new objects and move them in space.

% SD 2021-05-01 15:04:56 +0200: Here you could also describe how we simulate language and navigtaion: by generating questions and answers.