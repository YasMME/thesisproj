\documentclass{article}



\usepackage[utf8]{inputenc}

\usepackage{natbib}
\usepackage{hyperref}

\usepackage{graphicx}

\usepackage[colorinlistoftodos]{todonotes}

\usepackage{parskip}
\setlength{\parskip}{10pt} 

\usepackage{tikz}
\usetikzlibrary{arrows, decorations.markings}

\usepackage{chngcntr}
\counterwithout{figure}{section}



\begin{document}


\begin{titlepage}
  

\centering
  
  
{\scshape\LARGE Master thesis project proposal\\}
  
\vspace{0.5cm}
  
{\huge\bfseries Giving Information to Embodied Agents in Dialogue-Based Navigation Tasks\\}
  
\vspace{2cm}
  
{\Large Yasmeen Emampoor (gusemampya@student.gu.se)\\}
  
\vspace{1.0cm}
  
{\large Suggested Supervisor: Simon Dobnik (FLoV)\\}
  
\vspace{1.5cm}
  
  
{\large Relevant completed courses:\par}
{\itshape (DIT381, Algorithms for Machine Learning and Inference)\\ (DIT866, Applied Machine Learning) \\ (DIT869, Deep Machine Learning)}
  
\vspace{1.5cm}
  
\vfill

\vfill
  
{\large \today\\} 


\end{titlepage}
\section{Introduction}
Robots have the potential to be useful in many fields–they can perform tasks
that are dangerous for humans, as well as tasks that are simply tedious. These
robots are often operated based on pre-defined actions, or remotely. The potential increases in efficiency from being able to give a goal/task in natural language
and have the robot interpret and determine how to carry out this task are huge.
This would make robots more accessible generally, since it would reduce the
learning curve for operating them. A patient in a hospital would likely find it much easier to simply tell the robot 'I need water', rather than having to find the 'water' option on some sort of touchscreen interface.

\section{Context}
\subsection{Grounding}
A key challenge in creating robots that can interact with humans in natural language is grounding--connecting an external object to a word or phrase. This challenge also includes linking of actions to words. What does 'turn' mean? One place where the grounding problem can be seen is in image captioning, where models need to produce both the objects in the image, and their interactions with each other\cite{karpathy2014captioning}.

There is also currently research in grounding through interaction with the environment. Hermann et. al present a simulated embodied agent learning grounded language through a combination of unsupervised and reinforcement learning, with minimal prior knowledge\cite{hermann2017grounded}. Another approach with minimal prior knowledge is presented by Thomason et. al, where the robot learns through dialogue with a human\cite{thomason2019grounded}.

\subsection{Navigation}
Navigation is a common task for robots, a subset of instruction following; applications for robots include moving through a space recording or streaming video back to a human and collecting objects in a space to deliver to someone in a different space. 
\subsubsection{Dialogue}
Navigation based on dialogue is a current area of active research--datasets for a dialogue based navigation tasks are often collected as human-human dialogues, which are then used to train agents in replay or supervised learning scenarios. 

Cooperative Vision-and-Dialog Navigation is a dataset of over 2000 navigation dialogues\cite{thomason2019visionanddialog}. It was collected via Mechanical Turk crowd-sourcing; pairs used the Matterport3D simulator and a chat interface, with one person, the ’oracle’, able to see the ideal moves for the navigation task, giving natural language instructions to the other person, the ’navigator'. The navigator could ask clarifying questions. The oracle was also shown the navigator's current visual frame. 

A similar but smaller dataset is RobotSlang, consisting of 169 dialogues between a commander referencing a static map and a human driver only able to see the camera view of the robot they were controlling\cite{robotslang}. The commander relayed instructions to the driver, based on their understanding of where the robot was. The driver was able to ask clarifying questions, such as where exactly to turn, and the commander could periodically ask localization questions, such as what color wall the driver could see.

\subsection{Simulation}
Working with embodied agents is resource intensive and makes reproducibility difficult to impossible, so simulation is popular for research. 

AI Habitat is a simulation platform for working with embodied AI\cite{habitat19iccv}. It consists of two parts, Habitat-Sim, the 3D simulator, and Habitat-Lab, the library for embodied AI development. Habitat's current main focus is navigation, mainly through indoor spaces, but there is some ability for object interactions--for example moving a chair from one point to another. Habitat-PyRobot Bridge is a library, written by members of the Habitat team, to support the transfer of a simulated agent in Habitat to a physical robot\cite{Kadian_2020}. 

\section{Goals and Challenges}
For this project, the overarching question is: how can information learned in other contexts be used to improve performance of an agent in a navigation task?

Object navigation seems like a good candidate for transfer learning, since when humans are performing navigation tasks, they use a lot of information that they've learned from other contexts. Color, for example, or what room people typically use an item in. 

Also, datasets for dialogue navigation take a long time to collect, since they are
generated in real time, and the navigation tasks can take a while. Because of this, many navigation datasets are small, which limits the agent's knowledge, so being able to use disparate datasets would be beneficial. For example, maybe an agent could learn colors and positions from a dialogue about items on a table, and then would be able to apply that knowledge from the beginning when presented with dialogue navigating through a house. 

Another use for transfer learning is in the fact that a goal within this research is for these human-human dialogues to become human-robot dialogues, more flexible than those described in \cite{thomason2019grounded}. Transfer learning could aid in teaching the robot grammar and vocabulary, rather than limiting its knowledge of language to the navigation dialogue, making it easier to create a robot that could generate its own responses to instructions. 

\section{Approach}
I plan to use Habitat to simulate my navigation agents and environment, and the Cooperative Vision-and-Dialogue Navigation set as the main dataset for training and testing of the agents. 

I will also need to determine what models have been used for navigation agents, and what kinds of representations those models learn and use. These representations will affect what kinds of background knowledge could help the model--a mainly word based representation might benefit from pre-training with general natural language dialogues, a mainly visual one probably wouldn't.

Another thing I will need to identify is potential datasets or pre-trained models for background/external knowledge: navigation datasets in other scenarios (outside, for example), datasets of learning information like position in non-navigation scenarios ('what is to the left of the blue ball in this picture?', for example), models for image captioning, or general natural language dialogues.

Next would be to build and train a model for object navigation solely on the Cooperative Vision-and-Dialogue Navigation dataset, to compare with models incorporating background information. After this, I would need to build and train models incorporating background information, and compare them. 

%\section{References}
\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}