\documentclass{article}



\usepackage[utf8]{inputenc}

\usepackage{natbib}
\usepackage{hyperref}

\usepackage{graphicx}

\usepackage[colorinlistoftodos]{todonotes}

\usepackage{parskip}
\setlength{\parskip}{10pt} 

\usepackage{tikz}
\usetikzlibrary{arrows, decorations.markings}

\usepackage{chngcntr}
\counterwithout{figure}{section}



\begin{document}


\begin{titlepage}
  

\centering
  
  
{\scshape\LARGE Master thesis project proposal\\}
  
\vspace{0.5cm}
  
{\huge\bfseries Giving Information to Embodied Agents in Dialogue-Based Navigation Tasks\\}
  
\vspace{2cm}
  
{\Large Yasmeen Emampoor (gusemampya@student.gu.se)\\}
  
\vspace{1.0cm}
  
{\large Suggested Supervisor: Simon Dobnik (FLoV)\\}
  
\vspace{1.5cm}
  
  
{\large Relevant completed courses:\par}
{\itshape (DIT381, Algorithms for Machine Learning and Inference)\\ (DIT866, Applied Machine Learning) \\ (DIT869, Deep Machine Learning)}
  
\vspace{1.5cm}
  
\vfill

\vfill
  
{\large \today\\} 


\end{titlepage}
\section{Introduction}
Robots have the potential to be useful in many fields–they can perform tasks
that are dangerous for humans, as well as tasks that are simply tedious. These
robots are often operated based on pre-defined actions, or remotely. The potential increases in efficiency from being able to give a goal/task in natural language and have the robot interpret and determine how to carry out this task are huge.
This would make robots more accessible generally, since it would reduce the
learning curve for operating them. A patient in a hospital would likely find it much easier to simply tell the robot 'I need water', rather than having to find the 'water' option on some sort of touchscreen interface. It also would support situations where the instructions being given are too complex for this kind of interface--one example of this is human-robot teaming, in which groups of humans and robots cooperate to achieve tasks in situations such as disaster scenarios\cite{Kruijff-Korbayova:2015aa}. Communication in natural language can facilitate organisation where all of the humans are working with all of the robots, rather than an individual human having control over one or many robots. Situations like this require complex understanding of natural language instructions. 

% SD 2020-12-11 21:10:09 +0100: There may be cases where even more complex understanding of the natural language instructions is required which would be very complex for the person so "describe" using touch screen interface. For example, human-robot teaming: https://link.springer.com/article/10.1007/s13218-015-0352-5 \cite{Kruijff-Korbayova:2015aa} and There is a lot of research in this area how robots and humans could work together, see for example the references to this paper I found https://arxiv.org/pdf/2009.00288.pdf

\section{Context}
\subsection{Grounding}
A key challenge in creating robots that can interact with humans in natural language is grounding--connecting an external object to a word or phrase. This challenge also includes linking of actions to words. For example, what does 'turn' mean? Grounding is a challenge in a variety of tasks, such as image captioning, where models need to produce labels for both the objects in the image, and the objects' interactions with each other in natural language\cite{karpathy2014captioning}.

% SD 2020-12-11 21:25:12 +0100: Actually, the notion of grounding language in perception has been explored in robotics first and then moved into image captioning which is a much younger task: https://canvas.gu.se/courses/36167/assignments/75554 and https://canvas.gu.se/courses/36167/assignments/76897

Image captioning is a fairly new task, but grounding in the field of robotics has been an area of active research for a long time. In 2002, Lauria et al. presented a project in which a robot was given natural language instructions, which were mapped to procedures of identified primitives, such as 'turn'\cite{lauria2002}. More recently, Hermann et al. presented a simulated embodied agent learning grounded language through a combination of unsupervised and reinforcement learning, with minimal prior knowledge\cite{hermann2017grounded}. Another approach with minimal prior knowledge was presented by Thomason et al., where the robot learns through dialogue with a human\cite{thomason2019grounded}.

\subsection{Navigation}
Navigation is a common task for robots, a subset of instruction following; applications for robots include moving through a space recording or streaming video back to a human and collecting objects in a space to deliver to someone in a different space. The task can be split into two main challenges: self-localization and mapping\cite{Wallgrun:2007zr}. 

% SD 2020-12-11 21:37:27 +0100: Here you could expand that there is active research on how humans navigate and reason in space. Perhaps a paper by Diedrich Wolter: https://www.sfbtr8.spatial-cognition.de/project/r3/material/QSR_Challenges.pdf \cite{Wallgrun:2007zr}

\subsubsection{Dialogue}
Navigation based on dialogue is a current area of active research--datasets for a dialogue based navigation tasks are often collected as human-human dialogues, which are then used to train agents in replay or supervised learning scenarios. 

Cooperative Vision-and-Dialog Navigation is a dataset of over 2000 navigation dialogues\cite{thomason2019visionanddialog}. It was collected via Mechanical Turk crowd-sourcing; pairs used the Matterport3D simulator and a chat interface, with one person, the ’oracle’, able to see the ideal moves for the navigation task, giving natural language instructions to the other person, the ’navigator'. The navigator could ask clarifying questions. The oracle was also shown the navigator's current visual frame. 

A similar but smaller dataset is RobotSlang, consisting of 169 dialogues between a commander referencing a static map and a human driver only able to see the camera view of the robot they were controlling\cite{robotslang}. The commander relayed instructions to the driver, based on their understanding of where the robot was. The driver was able to ask clarifying questions, such as where exactly to turn, and the commander could periodically ask localization questions, such as what color wall the driver could see.

There has also been a lot of research on how humans actually communicate about directions. Humans have a number of strategies for communicating about routes, and which strategy someone will choose to use is often not immediately obvious\cite{Tenbrink:2010qf}. They may also choose to use multiple methods in conjunction. These strategies can relate to what perspective directions are given from or how much detail to give initially, among other things. Another point of consideration is how humans automatically adjust their strategies when interacting with a computer. Tenbrink et al. found that people gave generally sparser commands when interacting with a computer system than they did interacting with another human, even when not given instructions to do so\cite{Tenbrink:2010qf}. 
% SD 2020-12-11 21:42:35 +0100: Similarly, a lot of research has been done on the language side and dialogue. How humans generate descriptions in conversation that are understandable by conversational partners? What landmarks will they use? How will they structure the route description: it is unlikely that they will give a complex description (as you would get from Google straight away). The description will be dependent on who the conversational partner is and what task they are involved in which further complicated computational modelling on the natural language processing side. Perhaps, \cite{Tenbrink:2010qf,Fellner:2017aa,Janarthanam:2012aa}

\subsection{Simulation}
Working with embodied agents is resource intensive and makes reproducibility difficult to impossible, so simulation is beneficial for research. Within simulation, environments can be kept consistent, allowing for both reproducibility of an experiment and for comparison of different systems/methods. Simulation also allows for the reuse of datasets of human descriptions, which as discussed above, are time-consuming and expensive to produce.  

AI Habitat is a simulation platform for working with embodied AI\cite{habitat19iccv}. It consists of two parts, Habitat-Sim, the 3D simulator, and Habitat-Lab, the library for embodied AI development. Habitat's current main focus is navigation, mainly through indoor spaces, but there is some ability for object interactions--for example moving a chair from one point to another. Habitat-PyRobot Bridge is a library, written by members of the Habitat team, to support the transfer of a simulated agent in Habitat to a physical robot\cite{Kadian_2020}. 

% SD 2020-12-11 21:35:47 +0100: You could also expand the point about reproducibility earlier that working with simulation environments has benefits for research since different systems can be compared, and datasets of human descriptions (which are expensive to obtain) reused.

\section{Goals and Challenges}
For this project, the overarching question is: how can information learned in other contexts be used to improve performance of an agent in a navigation task? % SD 2020-12-11 22:04:22 +0100: Given this research quetsion it is good that you give more background about the challenges for navigation instructions as I commeneted above.

Object navigation seems like a good candidate for transfer learning, since when humans are performing navigation tasks, they use a lot of information that they've learned from other contexts. Color, for example, or what room people typically use an item in. 

Also, datasets for dialogue navigation take a long time to collect, since humans are generating the descriptions in real time, and humans are not necessarily very efficient. For an unsupervised learning approach, several thousand route instructions would be needed. % SD 2020-12-11 21:58:38 +0100: The bottle-neck is mainly due to human descriptions. These take time and several thousand route instructions would be required for an unsupervised approach.
Because of this, many navigation datasets are small, which limits the agent's knowledge, so being able to use disparate datasets would be beneficial. For example, maybe an agent could learn colors and positions from a dialogue about items on a table, and then would be able to apply that knowledge from the beginning when presented with dialogue navigating through a house. An agent could be trained on a corpora of dialogue from a different domain, and from this learn about human interaction strategies. The system could then be fine tuned on dialogue from the navigation domain, and adapt the knowledge about interaction to understanding route instructions. 

% SD 2020-12-11 22:06:15 +0100: We could train the system on corpora of dialogue from a different domain and then fine tune it on this domain. This way the system would already know something about the interaction strategies of humans; the leanring would then be adapting this knowledge to the route instruction scenario.

Another use for transfer learning is in the fact that a goal within this research is for these human-human dialogues to become human-robot dialogues, more flexible than those described in \cite{thomason2019grounded}. Transfer learning could aid in teaching the robot grammar and vocabulary, rather than limiting its knowledge of language to the navigation dialogue, making it easier to create a robot that could generate its own responses to instructions. 

\section{Approach}
I plan to use Habitat to simulate my navigation agents and environment, and the Cooperative Vision-and-Dialogue Navigation set as the main dataset for training and testing of the agents. 

I will also need to determine what models have been used for navigation agents, and what kinds of representations those models learn and use. These representations will affect what kinds of background knowledge could help the model--a mainly word based representation might benefit from pre-training with general natural language dialogues, a mainly visual one probably wouldn't.

Another thing I will need to identify is potential datasets or pre-trained models for background/external knowledge: navigation datasets in other scenarios (outside, for example), datasets of learning information like position in non-navigation scenarios ('what is to the left of the blue ball in this picture?', for example), models for image captioning, or general natural language dialogues.

Next would be to build and train a model for object navigation solely on the Cooperative Vision-and-Dialogue Navigation dataset, to compare with models incorporating background information. After this, I would need to build and train models incorporating background information, and compare them. 

Generated descriptions can be evaluated based on automatic metrics such as BLEU and CIDEr, as well as via crowdsourcing on platforms such as Mechanical Turk. The performance of the agents at the navigation task can be expressed as the number of steps the agent took over the shortest path (of course multiplied by if the robot ever found the object). 

% SD 2020-12-11 22:08:22 +0100: Looks good! Good luck!

%\section{References}
\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
