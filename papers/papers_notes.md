| Title | Authors | Focus | Model | Task | Notes | Code |
| ---|:---|:---|:---|:---|:---|:---|
|Habitat: A Platform for Embodied AI Research|<details><summary>authors</summary>Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra</details>|Platform||3D simulation|<https://aihabitat.org/>|<https://github.com/facebookresearch/habitat-lab>|
|Embodied Question Answering|Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra|Task, Model|Vision: CNN, Language: LSTMs, Navigation: LSTM(planner) and MLP(controller)|Embodied Question Answering|Baseline available in Habitat, <https://embodiedqa.org/>|<https://github.com/facebookresearch/EmbodiedQA>| 
|Multi-Target Embodied Question Answering|Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal, Tamara L. Berg, Dhruv Batra|Task, Model|Question-to-Program Generator: templated, Navigation:CNN Feature Extrator, LSTM, Controller: CNN, LSTM, VQA Module: FC + ReLU| Multitarget EQA|<https://youtu.be/pK5gYk9OgjE>||
|Visual question answering: A survey of methods and datasets|Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, Anton van den Hengel|Survey: Approaches, Datasets||VQA|||
|CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning|Justin Johnson, Li Fei-Fei, Bharath Hariharan, C. Lawrence Zitnick, Laurens van der Maaten, Ross Girshick|Dataset||Visual Question Answering|<https://cs.stanford.edu/people/jcjohns/clevr/>||
|Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments|Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee|||||<https://github.com/jacobkrantz/VLN-CE>|
|Robot Navigation with Language Pretraining and Stochastic Sampling|Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonaton Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah A. Smith, Yejin Choi|Pre-training|LSTM/GPT/BERT, LSTM|Vision and Language Navigation||<https://github.com/xjli/r2r_vln>|
|Learning to Explore Using Active Neural Slam|Devendra Singh Chaplot, Dhiraj Ghandi, Saurabh Gupta, Abhinav Gupta, Ruslan Salakhutdinov|Model||PointGoal|||
|Object Goal Navigation using Goal-Oriented Semantic Exploration|Devendra Singh Chaplot, Dhiraj Ghandi, Abhinav Gupta, Ruslan Salakhutdinov|Model||ObjectNav|||
|MAttNet: Modular Attention Network for Referring Expression Comprehension|Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L. Berg|Framework/Model|Language Attention Network: Embedding LSTM, FC layer + visual modules|Referring Expressions||<https://github.com/lichengunc/MAttNet>|
|Blindfold Baselines for Embodied QA|Ankesh Anand, Eugene Belilovsky, Kyle Kastner, Hugo Larochelle, Aaron Courville|Are Embodied QA tasks actually using the visual input?||Embodied QA||<https://github.com/ankeshanand/blindfold-baselines-eqa>|
|Situated Dialogue and Spatial Organization: What, Where... and Why?|Geert-Jan M. Kruijff, Hendrik Zender, Patric Jensfelt, and Henrik I. Christensen|Navigation, Human-Robot Interaction||Human Augmented Mapping, Situated Dialogue, Question Answering|||
|Robots That Use Language|Stefanie Tellex, Nakul Gopalan, Hadas Kress-Gazit, Cynthia Matuszek|Summary Paper|||Contains table of language grounding/robotics datasets with links||
|VL-BERT: Pre-training of Generic Visual-Linguistic Representations| Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai|Pre-training, Model||variety of Visual-Linguistic||https://github.com/jackroos/VL-BERT|
|DenseCap: Fully Convolutional Localization Networks for Dense Captioning|Justin Johnson, Andrej Karpathy, Li Fei-Fei|Task, Model||Image Captioning ("Dense Captioning")|<https://youtu.be/2wRnmRSrgCo>, <https://cs.stanford.edu/people/karpathy/densecap/>|<https://github.com/jcjohnson/densecap>|
|When an Image Tells a Story: The Role of Visual and Semantic Information for Generating Paragraph Descriptions|Nikolai Ilinykh, Simon Dobnik| Model||Generating paragraph descriptions of images|||
|Generating Question Relevant Captions to Aid Visual Question Answering|Jialin Wu, Zeyuan Hu and Raymond J. Mooney||||||

